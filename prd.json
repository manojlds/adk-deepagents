{
  "project": "Close All Review Gaps (REVIEW.md §4.1–§4.10)",
  "branchName": "ralph/close-review-gaps",
  "description": "Implement all 10 remaining gaps identified in REVIEW.md to bring feature parity from ~88% to ≥95%, including StoreBackend, image support, model-aware summarization, HITL, async backends, custom middleware, SubAgentSpec fields, skills error handling, write_file error codes, and integration tests.",
  "userStories": [
    {
      "id": "US-001",
      "title": "write_file Error Code Semantics",
      "description": "As a developer, I want write_file to return 'already_exists' instead of 'invalid_path' when a file already exists so that error messages are semantically correct.",
      "acceptanceCriteria": [
        "FileOperationError (or equivalent error type in backends/protocol.py or types.py) includes 'already_exists' as a valid error code",
        "StateBackend.write() returns error='already_exists' when a file already exists",
        "FilesystemBackend.write() returns error='already_exists' when a file already exists",
        "Existing unit tests are updated to expect the new error code",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-002",
      "title": "Skills Silent Failure Fix",
      "description": "As a developer, I want a clear error message when I pass skills=[...] but adk-skills-agent is not installed so that I know what dependency I'm missing.",
      "acceptanceCriteria": [
        "In graph.py, when skills parameter is provided and adk-skills-agent import fails, raise ImportError with message: 'adk-skills-agent is required for skills support. Install it with: pip install adk-skills-agent'",
        "When skills is NOT provided, the import failure is still silently ignored (library is optional)",
        "Unit test verifies the error is raised when skills are requested but import fails",
        "Unit test verifies no error when skills are not requested and import fails",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-003",
      "title": "Model-Aware Summarization Defaults",
      "description": "As a developer using models with varying context windows, I want summarization triggers to be computed based on the actual model's context window so that summarization fires at the right time.",
      "acceptanceCriteria": [
        "SummarizationConfig has an optional context_window: int | None field",
        "_resolve_context_window() in summarization.py checks config first, then falls back to a model-name lookup table (e.g., {'gemini-2.5-flash': 1_048_576, 'gemini-2.5-pro': 1_048_576, 'gpt-4o': 128_000})",
        "If model is not in lookup and config is not set, falls back to DEFAULT_CONTEXT_WINDOW",
        "Trigger and keep values are computed as fractions of the resolved context window (85% trigger, 10% keep — matching deepagents defaults)",
        "Unit tests verify correct resolution for known models, unknown models, and explicit config",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-004",
      "title": "Async Backend Methods — ABC Definition and Default Wrappers",
      "description": "As a developer using async tools, I want the Backend ABC to define async method signatures with default asyncio.to_thread() wrappers so that async ADK tools can call backends without blocking the event loop.",
      "acceptanceCriteria": [
        "Backend ABC in backends/protocol.py defines async methods: als_info, aread, awrite, aedit, agrep_raw, aglob_info",
        "Default implementations wrap sync methods via asyncio.to_thread()",
        "Unit tests verify async methods produce the same results as sync methods on a test backend",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-005",
      "title": "Async Backend Methods — StateBackend Overrides",
      "description": "As a developer, I want StateBackend to override async methods with direct async implementations (no thread needed since it's in-memory) so that in-memory operations are efficient.",
      "acceptanceCriteria": [
        "StateBackend overrides async methods with direct async implementations (no asyncio.to_thread needed)",
        "Unit tests verify StateBackend async methods produce the same results as sync methods",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Depends on US-004 (ABC async method definitions)"
    },
    {
      "id": "US-006",
      "title": "Async Backend Methods — CompositeBackend Delegation",
      "description": "As a developer, I want CompositeBackend to delegate to the resolved backend's async methods so that composed backends work correctly in async contexts.",
      "acceptanceCriteria": [
        "CompositeBackend delegates to the resolved backend's async methods",
        "FilesystemBackend uses the default asyncio.to_thread() wrappers (inherited from ABC)",
        "Unit tests verify CompositeBackend async delegation works correctly",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Depends on US-004 and US-005"
    },
    {
      "id": "US-007",
      "title": "StoreBackend (Cross-Thread Persistence)",
      "description": "As a developer building multi-session agents, I want a StoreBackend that persists files across different conversation sessions so that agents can share data across threads.",
      "acceptanceCriteria": [
        "StoreBackend class exists in adk_deepagents/backends/store.py",
        "Implements the full Backend ABC (ls, read, write, edit, grep, glob) including async methods",
        "Uses ADK's artifact storage or a shared dict-based store for persistence",
        "Supports namespace-based isolation (files are scoped by namespace prefix)",
        "Files written in session A can be read in session B (cross-thread)",
        "CompositeBackend can route to StoreBackend by path prefix",
        "Unit tests in tests/unit_tests/test_store_backend.py cover all operations",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Depends on US-004 (async ABC methods)"
    },
    {
      "id": "US-008",
      "title": "Image Support in read_file",
      "description": "As a developer building multimodal agents, I want read_file to detect image files and return base64-encoded multimodal content so that the agent can process images.",
      "acceptanceCriteria": [
        "read_file in tools/filesystem.py detects image extensions (.png, .jpg, .jpeg, .gif, .webp)",
        "Image files are read via backend.download_files() (binary read)",
        "Image content is base64-encoded and returned as a dict with {'type': 'image', 'media_type': 'image/png', 'data': '<base64>'}",
        "Non-image files continue to return text content as before",
        "Unit tests cover both image and non-image paths",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-009",
      "title": "Human-in-the-Loop (True Pause/Resume)",
      "description": "As a developer building approval workflows, I want the agent to truly pause execution when a tool requires human approval and resume only after approval is granted so that humans can review and approve/reject tool actions.",
      "acceptanceCriteria": [
        "before_tool_callback in callbacks/before_tool.py returns a response that halts the agent's execution loop (not just sets state)",
        "The halting mechanism is documented — either raising a custom exception, returning a specific ADK signal, or using ADK's native interrupt/yield mechanism",
        "Pending approval state includes: tool name, arguments, and a unique approval ID",
        "A resume_approval(approval_id, approved: bool, modified_args: dict | None) helper function or pattern is provided",
        "When approved, the tool executes with original (or modified) arguments",
        "When rejected, the agent receives a rejection message and continues without executing the tool",
        "If ADK does not support true interrupts, document the limitation clearly and implement the best available approximation",
        "Unit tests cover approve, reject, and modified-args paths",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Requires research into ADK's before_tool_callback return semantics"
    },
    {
      "id": "US-010",
      "title": "Custom Middleware Extensibility",
      "description": "As a developer extending adk-deepagents, I want to pass custom callback functions to create_deep_agent() so that I can add logging, monitoring, or custom prompt injection without modifying the library.",
      "acceptanceCriteria": [
        "create_deep_agent() in graph.py accepts an optional extra_callbacks parameter",
        "extra_callbacks is a dict with optional keys: before_agent, before_model, before_tool, after_tool",
        "Each extra callback is composed with the built-in callback — the built-in runs first, then the extra callback",
        "If the built-in callback returns a value that short-circuits (e.g., before_tool_callback returning a dict to skip the tool), the extra callback is NOT called",
        "Type signature: extra_callbacks: dict[str, Callable] | None = None",
        "Unit tests verify composition order and short-circuit behavior",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 10,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-011",
      "title": "SubAgentSpec Missing Fields",
      "description": "As a developer defining specialized sub-agents, I want SubAgentSpec to support skills and interrupt_on fields so that sub-agents can independently discover skills and have their own HITL configuration.",
      "acceptanceCriteria": [
        "SubAgentSpec in types.py adds optional fields: skills: list[str] and interrupt_on: dict[str, bool]",
        "When skills is set on a sub-agent, skill tools are added to that sub-agent (not shared from parent)",
        "When interrupt_on is set, the sub-agent gets its own before_tool_callback for HITL",
        "create_deep_agent() in graph.py processes these new fields when building sub-agents",
        "Support passing pre-built LlmAgent instances directly in the subagents list (equivalent to deepagents' CompiledSubAgent)",
        "Unit tests verify sub-agent creation with skills, interrupt_on, and pre-built agents",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 11,
      "passes": false,
      "notes": "Depends on US-002 (skills error handling) and US-009 (HITL)"
    },
    {
      "id": "US-012",
      "title": "Integration Tests — Setup and Filesystem Round-Trip",
      "description": "As a developer, I want integration test infrastructure and a filesystem round-trip test so that I have confidence the agent works end-to-end with a real LLM.",
      "acceptanceCriteria": [
        "Integration tests live in tests/integration_tests/",
        "litellm is added to dev dependencies in pyproject.toml",
        "pyproject.toml is updated with the integration marker definition",
        "Tests use LiteLlm from google.adk.models.lite_llm configured to hit OpenCode Zen endpoint (https://opencode.ai/zen/v1/chat/completions) with appropriate model",
        "API key is read from OPENCODE_API_KEY environment variable",
        "Tests are marked with @pytest.mark.integration so they can be run separately via uv run pytest -m integration",
        "Tests are skipped with a clear message if OPENCODE_API_KEY is not set",
        "Filesystem round-trip scenario: Agent creates a file via write_file, reads it back via read_file, edits it via edit_file, verifies final content",
        "uv run pytest passes (integration tests skipped without API key)",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 12,
      "passes": false,
      "notes": "Depends on all backend/tool stories being complete"
    },
    {
      "id": "US-013",
      "title": "Integration Tests — Sub-Agent Delegation",
      "description": "As a developer, I want an integration test that validates sub-agent delegation works end-to-end with a real LLM.",
      "acceptanceCriteria": [
        "Sub-agent delegation scenario: Main agent delegates a task to a sub-agent, verifies the sub-agent ran and returned a result",
        "Test is marked with @pytest.mark.integration",
        "Test is skipped with a clear message if OPENCODE_API_KEY is not set",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 13,
      "passes": false,
      "notes": "Depends on US-012 (integration test setup)"
    },
    {
      "id": "US-014",
      "title": "Integration Tests — Summarization Trigger and Memory Loading",
      "description": "As a developer, I want integration tests for summarization trigger and memory loading so that I have confidence these features work with a real LLM.",
      "acceptanceCriteria": [
        "Summarization trigger scenario: Send enough messages to trigger summarization, verify conversation history is preserved (not lost)",
        "Memory loading scenario: Agent loads an AGENTS.md file via memory config, verify the content appears in the agent's system prompt",
        "Both tests are marked with @pytest.mark.integration",
        "Tests are skipped with a clear message if OPENCODE_API_KEY is not set",
        "uv run pytest passes",
        "uv run ruff check . passes",
        "uv run ruff format --check . passes"
      ],
      "priority": 14,
      "passes": false,
      "notes": "Depends on US-012 (integration test setup) and US-003 (model-aware summarization)"
    }
  ]
}
